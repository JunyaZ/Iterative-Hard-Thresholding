{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection for Categorical Features using Iterative Hard Thresholding\n",
    "\n",
    "Feature selection is a very important task in machine learning tasks. Finding a relevant subset of features improves generalization, robustness to noise, and convergence to targets. A prominent technique in feature selection *Iterative Hard Thresholding (IHT)*. Although IHT is a powerful technique for feature selection, it is not designed for categorical features. In this notebook, I will give a walkthrough on **how to extend IHT to categorical datasets**. For validation, I will compare IHT to other machine learning techniques that also perform feature selections such as LASSO and Random Forest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Hard Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Consider a dataset $(\\mathbf{X_i}, y_i)$ for $i = [n] = 1,2,3 ... n$, where $\\mathbf{X_i} \\in \\mathbb{R}^m$ and $y_i \\in \\mathbb{R}.$ We seek to find a k-sparse vector $\\mathbf{B} \\in  \\mathbb{R}^m$ with the following objective: \n",
    "\n",
    "\n",
    "$$ \\min_{\\|\\mathbf{B}\\|_0 = k} \\| \\mathbf{y} - f(\\mathbf{X} ;\\mathbf{B}) \\|\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X} \\in \\mathbb{R}^{n \\times m}, \\mathbf{y} \\in  \\mathbb{R}^{n} $ denote the data matrix and the response vector. The $l_0$ norm denotes the number of non-zero elements in $\\mathbf{B}$ and $f$ is a given task-specific function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Intuitively, learning $\\mathbf{B}$ is equivalent to learning a set of $k$ relevant features that best recover the response vector. Thus, by solving this optimization problem, one can identify relevant features. \n",
    "\n",
    "IHT assumes there exists a linear relationship between a subset of the measured variable and the response vector , and seeks to retain the top-k values of the entire feature vector after each iteration. Let $\\mathbf{B}^0 = \\mathbf{0}$, IHT follows the update rule:\n",
    "\n",
    "$$\n",
    "\\mathbf{B}^{t+1} =  \\mathbf{H}_k(\\mathbf{B}^{t} - \\lambda \\bigtriangledown_{\\mathbf{B}^t} L(f(\\mathbf{X} ;\\mathbf{B}),y))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "import jax.numpy as NP\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 2000\n",
    "lr = 0.1\n",
    "s = 50\n",
    "dataPath =\"/home/tayo/Downloads/THY/IHT_data/molecularData.csv\"\n",
    "labelPath= \"/home/tayo/Downloads/THY/IHT_data/molecularLabel.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt(dataPath, dtype= str, delimiter = \",\")\n",
    "label= np.genfromtxt(labelPath, dtype = str, delimiter = \",\")\n",
    "\n",
    "classW = class_weight.compute_class_weight('balanced', np.unique(label), label)\n",
    "uniqueClasses = np.unique(label).tolist()\n",
    "numClasses = len(uniqueClasses)\n",
    "classWL = np.array([classW[uniqueClasses.index(i)] for i in label]).reshape(-1,1) #Calculate weight class for each sample\n",
    "label = LabelEncoder().fit_transform(label)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, label, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tayo/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/tayo/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py:468: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n",
      "/home/tayo/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found unknown categories [0.0, 1.0] in column 0 during transform",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-bdb8701c6917>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mDIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_trainC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOHC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mX_testC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOHC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdataC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOHC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    611\u001b[0m                                        copy=True)\n\u001b[1;32m    612\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform_new\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0mX_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, handle_unknown)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     msg = (\"Found unknown categories {0} in column {1}\"\n\u001b[1;32m    106\u001b[0m                            \" during transform\".format(diff, i))\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0;31m# Set the problematic rows to an acceptable value and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found unknown categories [0.0, 1.0] in column 0 during transform"
     ]
    }
   ],
   "source": [
    "OHC = OneHotEncoder().fit(data)\n",
    "OHCL = OneHotEncoder().fit(label.reshape(-1,1))\n",
    "classCount = list(map(len, OHC.categories_)) #Number of all possible values for each categorical feature\n",
    "indices = np.cumsum(classCount) # For indexing between coefficient and categorical features\n",
    "DIM = indices[-1]\n",
    "indices = np.insert(indices,0, 0)\n",
    "X_trainC = OHC.transform(X_train).toarray()\n",
    "X_testC = OHC.transform(X_test).toarray()\n",
    "dataC = OHC.transform(data).toarray()\n",
    "X_current, Y_current, weight =  None, None, None #Placeholder for batch data\n",
    "num_train = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregateFeature(gradients, numSplit):\n",
    "    gradients = gradients.flatten()\n",
    "    sumList = []\n",
    "    current = 0\n",
    "    for i in numSplit:\n",
    "        sumList.append(np.mean(np.abs(gradients[current: current + i ])))\n",
    "        current = current + i\n",
    "    return sumList\n",
    "\n",
    "def thresholding(coeff):\n",
    "    copyCof = coeff[:]\n",
    "    coeff = np.sum(np.abs(np.array(coeff)), axis = 0) \n",
    "    sum_coeff = aggregateFeature(coeff, classCount)\n",
    "    rankingBest = np.argsort(np.abs(sum_coeff)).ravel()[-s:]\n",
    "    if rankingBest.shape[0] < s:\n",
    "        print(\"rankBest less than s features\")\n",
    "    selected = set([i for j in rankingBest for i in range(indices[j], indices[j+1])]) #List of selected coefficient\n",
    "    coeff = coeff.flatten()\n",
    "    notSelected = list(set(range(len(coeff))).difference(selected))\n",
    "    copyCof[:,notSelected] = 0\n",
    "    return copyCof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateBatch():\n",
    "    #int(num_train / 5)\n",
    "    index = np.random.choice(num_train, size =int(num_train / 1), replace =  False)\n",
    "    global X_current\n",
    "    global Y_current\n",
    "    global weight\n",
    "    X_current = X_trainC[index]\n",
    "    Y_current = y_train[index].reshape(-1,1)\n",
    "    weight = classWL[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(coeff):  # Define the softmax function\n",
    "  y = NP.exp(NP.dot(X_current, coeff.T))\n",
    "  s = NP.expand_dims(NP.sum(y, axis = 1), 1)\n",
    "  y = y / s\n",
    "  Y_currentT = OHCL.transform(Y_current).toarray()\n",
    "  label_logprobs = NP.multiply(NP.log(y) , Y_currentT) #+ NP.multiply(NP.log(1 - y) , (1 - Y_currentT))\n",
    "  label_logprobs = weight * label_logprobs\n",
    "  return -NP.mean(label_logprobs)\n",
    "\n",
    "  \n",
    "def regressionTest(coeff):  # Define a function\n",
    "  y = np.exp(np.dot(X_testC, coeff.T))\n",
    "  s = np.expand_dims(np.sum(y, axis = 1), 1)\n",
    "  y = y / s \n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tayo/anaconda3/lib/python3.6/site-packages/jax/lib/xla_bridge.py:146: UserWarning: No GPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU found, falling back to CPU.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.4223118 Accuracy: 0.25914315569487983\n",
      "Training error: 0.17078292 Accuracy: 0.9456635318704284\n",
      "Training error: 0.11761189 Accuracy: 0.9498432601880877\n",
      "Training error: 0.094772264 Accuracy: 0.9498432601880877\n",
      "Training error: 0.08183824 Accuracy: 0.9540229885057471\n",
      "Training error: 0.0733718 Accuracy: 0.955067920585162\n",
      "Training error: 0.067319535 Accuracy: 0.9571577847439916\n",
      "Training error: 0.06273099 Accuracy: 0.9571577847439916\n",
      "Training error: 0.059103437 Accuracy: 0.9571577847439916\n",
      "Training error: 0.056144744 Accuracy: 0.9561128526645768\n",
      "Training error: 0.05367279 Accuracy: 0.9561128526645768\n",
      "Training error: 0.05156764 Accuracy: 0.955067920585162\n",
      "Training error: 0.049746804 Accuracy: 0.955067920585162\n",
      "Training error: 0.04815152 Accuracy: 0.9561128526645768\n",
      "Training error: 0.04673871 Accuracy: 0.9561128526645768\n",
      "Training error: 0.04547589 Accuracy: 0.9561128526645768\n",
      "Training error: 0.04433818 Accuracy: 0.9571577847439916\n",
      "Training error: 0.043306064 Accuracy: 0.9592476489028213\n",
      "Training error: 0.042364065 Accuracy: 0.9582027168234065\n",
      "Training error: 0.04149966 Accuracy: 0.9582027168234065\n"
     ]
    }
   ],
   "source": [
    "cof = np.zeros((numClasses, DIM))\n",
    "grad_regression  = grad(regression)\n",
    "for i in range(epoch):\n",
    "    generateBatch()\n",
    "    trainError =  regression(cof)\n",
    "    acc = accuracy_score(np.argmax(regressionTest(cof), axis = 1) , y_test)\n",
    "    if i % 100 == 0:\n",
    "        print(\"Training error:\", trainError, \"Accuracy:\",acc )\n",
    "    gradient = np.array(grad_regression(cof))\n",
    "    cof = thresholding(cof - lr * gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9373040752351097\n",
      "0.9247648902821317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tayo/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/home/tayo/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataC, label, random_state=1, test_size = 0.3)\n",
    "        \n",
    "\n",
    "clf = linear_model.SGDClassifier(loss ='log', penalty = 'l1')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(accuracy_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(accuracy_score(y_test, clf.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
