{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection for Categorical Features using Iterative Hard Thresholding\n",
    "\n",
    "Feature selection is a very important task in machine learning tasks. Finding a relevant subset of features improves generalization, robustness to noise, and convergence to targets. A prominent technique in feature selection *Iterative Hard Thresholding (IHT)*. Although IHT is a powerful technique for feature selection, it is not designed for categorical features. In this notebook, I will give a walkthrough on **how to extend IHT to categorical datasets**. For validation, I will compare IHT to other machine learning techniques that also perform feature selections such as LASSO and Random Forest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Hard Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Consider a dataset $(\\mathbf{X_i}, y_i)$ for $i = [n] = 1,2,3 ... n$, where $\\mathbf{X_i} \\in \\mathbb{R}^m$ and $y_i \\in \\mathbb{R}.$ We seek to find a k-sparse vector $\\mathbf{B} \\in  \\mathbb{R}^m$ with the following objective: \n",
    "\n",
    "\n",
    "$$ \\min_{\\|\\mathbf{B}\\|_0 = k} \\| \\mathbf{y} - f(\\mathbf{X} ;\\mathbf{B}) \\|\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X} \\in \\mathbb{R}^{n \\times m}, \\mathbf{y} \\in  \\mathbb{R}^{n} $ denote the data matrix and the response vector. The $l_0$ norm denotes the number of non-zero elements in $\\mathbf{B}$ and $f$ is a given task-specific function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Intuitively, learning $\\mathbf{B}$ is equivalent to learning a set of $k$ relevant features that best recover the response vector. Thus, by solving this optimization problem, one can identify relevant features. \n",
    "\n",
    "IHT assumes there exists a linear relationship between a subset of the measured variable and the response vector , and seeks to retain the top-k values of the entire feature vector after each iteration. Let $\\mathbf{B}^0 = \\mathbf{0}$, IHT follows the update rule:\n",
    "\n",
    "$$\n",
    "\\mathbf{B}^{t+1} =  \\mathbf{H}_k(\\mathbf{B}^{t} - \\lambda \\bigtriangledown_{\\mathbf{B}^t} L(f(\\mathbf{X} ;\\mathbf{B}),y))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "import jax.numpy as NP\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 2000\n",
    "lr = 0.1\n",
    "s = 50\n",
    "dataPath =\"molecularData.csv\"\n",
    "labelPath= \"molecularLabel.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt(dataPath, dtype= str, delimiter = \",\")\n",
    "label= np.genfromtxt(labelPath, dtype = str, delimiter = \",\")\n",
    "\n",
    "classW = class_weight.compute_class_weight('balanced', np.unique(label), label)\n",
    "uniqueClasses = np.unique(label).tolist()\n",
    "numClasses = len(uniqueClasses)\n",
    "classWL = np.array([classW[uniqueClasses.index(i)] for i in label]).reshape(-1,1) #Calculate weight class for each sample\n",
    "label = LabelEncoder().fit_transform(label)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, label, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Junya/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "OHC = OneHotEncoder().fit(data)\n",
    "OHCL = OneHotEncoder().fit(label.reshape(-1,1))\n",
    "classCount = list(map(len, OHC.categories_)) #Number of all possible values for each categorical feature\n",
    "indices = np.cumsum(classCount) # For indexing between coefficient and categorical features\n",
    "DIM = indices[-1]\n",
    "indices = np.insert(indices,0, 0)\n",
    "X_trainC = OHC.transform(X_train).toarray()\n",
    "X_testC = OHC.transform(X_test).toarray()\n",
    "dataC = OHC.transform(data).toarray()\n",
    "X_current, Y_current, weight =  None, None, None #Placeholder for batch data\n",
    "num_train = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregateFeature(gradients, numSplit):\n",
    "    gradients = gradients.flatten()\n",
    "    sumList = []\n",
    "    current = 0\n",
    "    for i in numSplit:\n",
    "        sumList.append(np.mean(np.abs(gradients[current: current + i ])))\n",
    "        current = current + i\n",
    "    return sumList\n",
    "\n",
    "def thresholding(coeff):\n",
    "    copyCof = coeff[:]\n",
    "    coeff = np.sum(np.abs(np.array(coeff)), axis = 0) \n",
    "    sum_coeff = aggregateFeature(coeff, classCount)\n",
    "    rankingBest = np.argsort(np.abs(sum_coeff)).ravel()[-s:]\n",
    "    if rankingBest.shape[0] < s:\n",
    "        print(\"rankBest less than s features\")\n",
    "    selected = set([i for j in rankingBest for i in range(indices[j], indices[j+1])]) #List of selected coefficient\n",
    "    coeff = coeff.flatten()\n",
    "    notSelected = list(set(range(len(coeff))).difference(selected))\n",
    "    copyCof[:,notSelected] = 0\n",
    "    return copyCof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateBatch():\n",
    "    #int(num_train / 5)\n",
    "    index = np.random.choice(num_train, size =int(num_train / 1), replace =  False)\n",
    "    global X_current\n",
    "    global Y_current\n",
    "    global weight\n",
    "    X_current = X_trainC[index]\n",
    "    Y_current = y_train[index].reshape(-1,1)\n",
    "    weight = classWL[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(coeff):  # Define the softmax function\n",
    "  y = NP.exp(NP.dot(X_current, coeff.T))\n",
    "  s = NP.expand_dims(NP.sum(y, axis = 1), 1)\n",
    "  y = y / s\n",
    "  Y_currentT = OHCL.transform(Y_current).toarray()\n",
    "  label_logprobs = NP.multiply(NP.log(y) , Y_currentT) #+ NP.multiply(NP.log(1 - y) , (1 - Y_currentT))\n",
    "  label_logprobs = weight * label_logprobs\n",
    "  return -NP.mean(label_logprobs)\n",
    "\n",
    "  \n",
    "def regressionTest(coeff):  # Define a function\n",
    "  y = np.exp(np.dot(X_testC, coeff.T))\n",
    "  s = np.expand_dims(np.sum(y, axis = 1), 1)\n",
    "  y = y / s \n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Junya/anaconda3/lib/python3.7/site-packages/jax/lib/xla_bridge.py:144: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.4223118 Accuracy: 0.25914315569487983\n",
      "Training error: 0.1705807 Accuracy: 0.9456635318704284\n",
      "Training error: 0.11736325 Accuracy: 0.9498432601880877\n",
      "Training error: 0.09449802 Accuracy: 0.9498432601880877\n",
      "Training error: 0.08155732 Accuracy: 0.9508881922675027\n",
      "Training error: 0.073095605 Accuracy: 0.955067920585162\n",
      "Training error: 0.06705454 Accuracy: 0.955067920585162\n",
      "Training error: 0.0624808 Accuracy: 0.955067920585162\n",
      "Training error: 0.058869965 Accuracy: 0.9561128526645768\n",
      "Training error: 0.055928886 Accuracy: 0.9571577847439916\n",
      "Training error: 0.053474702 Accuracy: 0.9571577847439916\n",
      "Training error: 0.05138715 Accuracy: 0.9571577847439916\n",
      "Training error: 0.049583487 Accuracy: 0.9571577847439916\n",
      "Training error: 0.048004765 Accuracy: 0.9571577847439916\n",
      "Training error: 0.04660785 Accuracy: 0.9582027168234065\n",
      "Training error: 0.0453602 Accuracy: 0.9561128526645768\n",
      "Training error: 0.04423694 Accuracy: 0.9561128526645768\n",
      "Training error: 0.043218527 Accuracy: 0.9561128526645768\n",
      "Training error: 0.042289518 Accuracy: 0.9561128526645768\n",
      "Training error: 0.041437417 Accuracy: 0.9571577847439916\n"
     ]
    }
   ],
   "source": [
    "cof = np.zeros((numClasses, DIM))\n",
    "grad_regression  = grad(regression)\n",
    "for i in range(epoch):\n",
    "    generateBatch()\n",
    "    trainError =  regression(cof)\n",
    "    acc = accuracy_score(np.argmax(regressionTest(cof), axis = 1) , y_test)\n",
    "    if i % 100 == 0:\n",
    "        print(\"Training error:\", trainError, \"Accuracy:\",acc )\n",
    "    gradient = np.array(grad_regression(cof))\n",
    "    cof = thresholding(cof - lr * gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9414838035527691\n",
      "0.9310344827586207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Junya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/Junya/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataC, label, random_state=1, test_size = 0.3)\n",
    "        \n",
    "\n",
    "clf = linear_model.SGDClassifier(loss ='log', penalty = 'l1')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(accuracy_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(accuracy_score(y_test, clf.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
