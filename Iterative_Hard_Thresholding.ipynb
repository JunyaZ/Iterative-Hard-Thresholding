{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection for Categorical Features using Iterative Hard Thresholding\n",
    "\n",
    "Feature selection is a very important task in machine learning tasks. Finding a relevant subset of features improves generalization, robustness to noise, and convergence to targets. A prominent technique in feature selection *Iterative Hard Thresholding (IHT)*. Although IHT is a powerful technique for feature selection, it is not designed for categorical features. In this notebook, I will give a walkthrough on **how to extend IHT to categorical datasets**. For validation, I will compare IHT to other machine learning techniques that also perform feature selections such as LASSO and Random Forest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Hard Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Consider a dataset $(\\mathbf{X_i}, y_i)$ for $i = [n] = 1,2,3 ... n$, where $\\mathbf{X_i} \\in \\mathbb{R}^m$ and $y_i \\in \\mathbb{R}.$ We seek to find a k-sparse vector $\\mathbf{B} \\in  \\mathbb{R}^m$ with the following objective: \n",
    "\n",
    "\n",
    "$$ \\min_{\\|\\mathbf{B}\\|_0 = k} \\| \\mathbf{y} - f(\\mathbf{X} ;\\mathbf{B}) \\|\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X} \\in \\mathbb{R}^{n \\times m}, \\mathbf{y} \\in  \\mathbb{R}^{n} $ denote the data matrix and the response vector. The $l_0$ norm denotes the number of non-zero elements in $\\mathbf{B}$ and $f$ is a given task-specific function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Intuitively, learning $\\mathbf{B}$ is equivalent to learning a set of $k$ relevant features that best recover the response vector. Thus, by solving this optimization problem, one can identify relevant features. \n",
    "\n",
    "IHT assumes there exists a linear relationship between a subset of the measured variable and the response vector , and seeks to retain the top-k values of the entire feature vector after each iteration. Let $\\mathbf{B}^0 = \\mathbf{0}$, IHT follows the update rule:\n",
    "\n",
    "$$\n",
    "\\mathbf{B}^{t+1} =  \\mathbf{H}_k(\\mathbf{B}^{t} - \\lambda \\bigtriangledown_{\\mathbf{B}^t} L(f(\\mathbf{X} ;\\mathbf{B}),y))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jaxlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5cbf1d0ff5e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mNP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\jax\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m  \u001b[1;31m# side-effecting import sets up operator overloads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\jax\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m from .util import (unzip2, unzip3, curry, partial, safe_map, safe_zip,\n\u001b[0;32m     45\u001b[0m                    WrapHashably, prod)\n\u001b[1;32m---> 46\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxla_bridge\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcanonicalize_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mabstract_arrays\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mShapedArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0minterpreters\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpartial_eval\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\jax\\lib\\xla_bridge.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0monp\u001b[0m  \u001b[1;31m# 'onp' rather than 'np' to distinguish from autograd.numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mjaxlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0m_minimum_jaxlib_version\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jaxlib'"
     ]
    }
   ],
   "source": [
    "from jax import grad\n",
    "import jax.numpy as NP\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 200\n",
    "lr = 0.01\n",
    "s = 50\n",
    "dataPath =\"molecularData.csv\"\n",
    "labelPath= \"molecularLabel.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt(dataPath, dtype= str, delimiter = \",\")\n",
    "label= np.genfromtxt(labelPath, dtype = str, delimiter = \",\")\n",
    "\n",
    "classW = class_weight.compute_class_weight('balanced', np.unique(label), label)\n",
    "uniqueClasses = np.unique(label).tolist()\n",
    "classWL = np.array([classW[uniqueClasses.index(i)] for i in label]).reshape(-1,1) #Calculate weight class for each sample\n",
    "label = LabelEncoder().fit_transform(label)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, label, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OHC = OneHotEncoder().fit(data)\n",
    "OHCL = OneHotEncoder().fit(label.reshape(-1,1))\n",
    "classCount = list(map(len, OHC.categories_)) #Number of all possible values for each categorical feature\n",
    "indices = np.cumsum(classCount) # For indexing between coefficient and categorical features\n",
    "DIM = indices[-1]\n",
    "indices = np.insert(indices,0, 0)\n",
    "X_trainC = OHC.transform(X_train).toarray()\n",
    "X_testC = OHC.transform(X_test).toarray()\n",
    "dataC = OHC.transform(data).toarray()\n",
    "X_current, Y_current, weight =  None, None, None #Placeholder for batch data\n",
    "num_train = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregateFeature(gradients, numSplit):\n",
    "    gradients = gradients.flatten()\n",
    "    sumList = []\n",
    "    current = 0\n",
    "    for i in numSplit:\n",
    "        sumList.append(np.mean(np.abs(gradients[current: current + i ])))\n",
    "        current = current + i\n",
    "    return sumList\n",
    "\n",
    "def thresholding(coeff):\n",
    "    copyCof = coeff[:]\n",
    "    coeff = np.sum(np.abs(np.array(coeff)), axis = 0) \n",
    "    sum_coeff = aggregateFeature(coeff, classCount)\n",
    "    rankingBest = np.argsort(np.abs(sum_coeff)).ravel()[-s:]\n",
    "    if rankingBest.shape[0] < s:\n",
    "        print(\"rankBest less than s features\")\n",
    "    selected = set([i for j in rankingBest for i in range(indices[j], indices[j+1])]) #List of selected coefficient\n",
    "    coeff = coeff.flatten()\n",
    "    notSelected = list(set(range(len(coeff))).difference(selected))\n",
    "    copyCof[:,notSelected] = 0\n",
    "    return copyCof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateBatch():\n",
    "    #int(num_train / 5)\n",
    "    index = np.random.choice(num_train, size =int(num_train / 1), replace =  False)\n",
    "    global X_current\n",
    "    global Y_current\n",
    "    global weight\n",
    "    X_current = X_trainC[index]\n",
    "    Y_current = y_train[index].reshape(-1,1)\n",
    "    weight = classWL[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(coeff):  # Define the softmax function\n",
    "  y = NP.exp(NP.dot(X_current, coeff.T))\n",
    "  s = NP.expand_dims(NP.sum(y, axis = 1), 1)\n",
    "  y = y / s\n",
    "  Y_currentT = OHCL.transform(Y_current).toarray()\n",
    "  label_logprobs = NP.multiply(NP.log(y) , Y_currentT) #+ NP.multiply(NP.log(1 - y) , (1 - Y_currentT))\n",
    "  label_logprobs = weight * label_logprobs\n",
    "  return -NP.mean(label_logprobs)\n",
    "\n",
    "  \n",
    "def regressionTest(coeff):  # Define a function\n",
    "  y = np.exp(np.dot(X_testC, coeff.T))\n",
    "  s = np.expand_dims(np.sum(y, axis = 1), 1)\n",
    "  y = y / s \n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cof = np.zeros((numClasses, DIM))\n",
    "grad_regression  = grad(regression)\n",
    "for i in range(epoch):\n",
    "    generateBatch()\n",
    "    trainError =  regression(cof)\n",
    "    acc = accuracy_score(np.argmax(regressionTest(cof), axis = 1) , y_test)\n",
    "    print(\"Training error:\", trainError, \"Accuracy:\",acc )\n",
    "    gradient = np.array(grad_regression(cof))\n",
    "    cof = thresholding(cof - lr * gradient)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
